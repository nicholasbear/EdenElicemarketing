{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEM.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZRF5x_-WZiw"
      },
      "outputs": [],
      "source": [
        "#패키지 설치\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3\n",
        "!pip install torch\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#라이브러리,모델 설치\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, notebook\n",
        "\n",
        "from torch.nn import init\n",
        "\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GroupShuffleSplit,StratifiedKFold\n",
        "import random\n",
        "import gc\n",
        "import unicodedata\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "JIrfD3bMXLSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################\n",
        "############################################################################\n",
        "############################################################################\n",
        "\n",
        "##GPU 사용 시\n",
        "# there are totally five GPU in server,we can routed to 0:4.\n",
        "device = torch.device(\"cuda:4\")\n",
        "gc.collect()\n",
        "\n",
        "#BERT 모델 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "metadata": {
        "id": "Pg4JOTfqXNe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KoBert 모델 클래스 생성"
      ],
      "metadata": {
        "id": "AhqqzJRnYkvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx,label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "        \n",
        "        self.sentences = []\n",
        "        self.labels = []\n",
        "#         for i in dataset:\n",
        "#             self.sentences.append(transform([i[sent_idx]]))\n",
        "#             self.labels.append(i[label_idx])\n",
        "        for i in dataset:\n",
        "            if len(i[sent_idx])<=max_len:\n",
        "                self.sentences.append(transform([i[sent_idx]]))\n",
        "                self.labels.append(i[label_idx])\n",
        "            else:\n",
        "                self.sentences.append(transform([i[sent_idx][:max_len]]))\n",
        "                self.labels.append(i[label_idx])\n",
        "        \n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n",
        "    \n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=11, #클래스 수\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "zP74g0FjXO9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define hyper parameter for finetuning\n",
        "max_len = 512\n",
        "batch_size = 6\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 20\n",
        "max_grad_norm = 1\n",
        "log_interval = 20\n",
        "learning_rate =  5e-6  #5e-5  2e-5\n",
        "num_workers = 2\n",
        "n_splits = 5\n",
        "model_name = 'kobertbest_512.pt'\n",
        "\n",
        "device = torch.device(\"cuda:4\")\n",
        "\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "# torch.save(model, 'kobert.pt')\n",
        "\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "torch.save({'modelA':model.state_dict(),'optimizerA':optimizer.state_dict()},'kobert.pt')\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "##################################################################################################\n",
        "# train test validate split\n",
        "\n",
        "# df_raw = pd.read_csv(\"extract100datasetmanual.csv?dl=1\")\n",
        "# df_raw = pd.read_csv(\"dataset200_12manually.csv\")\n",
        "\n",
        "#df_raw = pd.read_csv(\"12classesdatsetnew.csv\",index_col=False )\n",
        "\n",
        "df_raw = pd.read_excel('11classesdatsetnew_20220208.xlsx', engine='openpyxl', sheet_name=\"classesdatsetnew\", header=0)\n",
        "\n",
        "# df_raw = pd.read_csv(\"dataset200_13manually2.csv\")\n",
        "# print(df_raw)\n",
        "# categorylist = [\"화장품\",\"핫플레이스\",\"요리음식\",\"여행아웃도어\",\"인테리어\",\"엔터테인먼트\"]\n",
        "categorylist = [\"화장품\",\"패션\",\"요리음식\",\"여행아웃도어\",\"인테리어\",\"엔터테인먼트\",\"육아\",\"아이티\",\"자동차\",\"헬스/피트니스\",\"반려동물\"]\n",
        "temp_label=[]\n",
        "for i in df_raw['label']:\n",
        "    temp_label.append(categorylist.index(i))\n",
        "df_raw['label']=temp_label\n",
        "\n",
        "#데이터 test/val 분할\n",
        "train_test_ratio = 0.8\n",
        "df_train_list =[None]*11\n",
        "df_test_list = [None]*11\n",
        "\n",
        "df_file =[None] *11\n",
        "\n",
        "for i, labeli in enumerate(range(len(categorylist))):\n",
        "    df_file[i] = df_raw[df_raw['label'] == labeli]\n",
        "\n",
        "for i, dffilei in enumerate(df_file):\n",
        "    df_train_list[i],df_test_list[i] = train_test_split(dffilei,train_size = train_test_ratio, random_state = 1)\n",
        "\n",
        "df_train = pd.concat([trainlist for trainlist in df_train_list],ignore_index=True,sort=False)\n",
        "df_test = pd.concat([dftest for dftest in df_test_list],ignore_index=True,sort=False)\n",
        "\n",
        "import re\n",
        "\n",
        "df_traindata = df_train.reindex(columns=['label', 'caption'])\n",
        "# df_traindata = df_train.reindex(columns=['label', 'hashtag'])\n",
        "# print(df_traindata)\n",
        "df_trainlabel = df_train.reindex(columns=['label'])\n",
        "np_data = df_traindata.to_numpy()\n",
        "np_label = df_trainlabel.to_numpy()\n",
        "\n",
        "for i in range(len(np_data)):\n",
        "    np_data[i][1] = unicodedata.normalize('NFC',np_data[i][1])\n",
        "    np_data[i][1] = ' '.join(re.compile('[가-힣]+').findall(np_data[i][1]))\n",
        "\n",
        "df_testdata = df_test.reindex(columns=['label', 'caption'])\n",
        "# df_testdata = df_test.reindex(columns=['label', 'hashtag'])\n",
        "df_testlabel = df_test.reindex(columns=['label'])\n",
        "np_testdata = df_testdata.to_numpy()\n",
        "np_testlabel = df_testlabel.to_numpy()\n",
        "\n",
        "for i in range(len(np_testdata)):\n",
        "    np_testdata[i][1] = unicodedata.normalize('NFC',np_testdata[i][1])\n",
        "    np_testdata[i][1] = ' '.join(re.compile('[가-힣]+').findall(np_testdata[i][1]))\n",
        "\n",
        "print(np_testdata)\n"
      ],
      "metadata": {
        "id": "LAWgLjsJXQG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groups = np.array([random.randint(0,6) for i in range(0,600)])\n",
        "# gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
        "skf = StratifiedKFold(n_splits=n_splits)\n",
        "#StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "nsplit =0\n",
        "oldaccu = 0.0\n",
        "trainarray = []\n",
        "testarray =[]\n",
        "# for trainidx, testidx in gss.split(np_data, np_label, groups=groups):\n",
        "for trainidx,testidx in skf.split(np_data,np_label):\n",
        "  # print(\"TRAIN:\", trainidx, \"TEST:\", testidx)\n",
        "  x_train,x_test,y_train,y_test = np_data[trainidx],np_data[testidx],np_label[trainidx],np_label[testidx]\n",
        "  # print(x_train)\n",
        "  # print(x_test)\n",
        "  # print(\"%s %s\" % (trainidx,testidx))\n",
        "  data_train = BERTDataset(x_train, 1, 0, tok, max_len, True, False)\n",
        "#   print(\"trainidx: \", len(trainidx))\n",
        "#   print(\"x_train: \", len(x_train))\n",
        "#   print(\"data_train: \", len(data_train))\n",
        "  data_test = BERTDataset(x_test, 1, 0, tok, max_len, True, False)\n",
        "  train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=num_workers) # before the num_workers = 5\n",
        "  test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, shuffle=True, num_workers=num_workers)  \n",
        "  # load the initial model parameters\n",
        "  checkpoint = torch.load('kobert.pt')\n",
        "  model.load_state_dict(checkpoint['modelA'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizerA'])\n",
        "  model.to(device)\n",
        "\n",
        "  t_total = len(train_dataloader) * num_epochs\n",
        "  warmup_step = int(t_total * warmup_ratio)\n",
        "  scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "  trainacclist =[]\n",
        "  testacclist =[]\n",
        "  # model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "  # optimizer.step()\n",
        "  # print(\"train_dataloader: \", len(train_dataloader))\n",
        " \n",
        "  for e in range(num_epochs):\n",
        "      train_acc = 0.0\n",
        "      test_acc = 0.0\n",
        "      model.train()\n",
        "      for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(notebook.tqdm(train_dataloader)):\n",
        "          optimizer.zero_grad()\n",
        "          token_ids = token_ids.long().to(device)\n",
        "          segment_ids = segment_ids.long().to(device)\n",
        "          valid_length= valid_length\n",
        "          label = label.long().to(device)\n",
        "          out = model(token_ids, valid_length, segment_ids)\n",
        "          loss = loss_fn(out, label)\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "          optimizer.step()\n",
        "          scheduler.step()  # Update learning rate schedule\n",
        "          train_acc += calc_accuracy(out, label)\n",
        "          if batch_id % log_interval == 0:\n",
        "              # print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "              print('Epoch {}/{} Batch {}/{} Cost: {:.6f} Train Acc {}'.format(e+1, num_epochs, batch_id+1, len(train_dataloader), loss.item(), train_acc / (batch_id+1)))\n",
        "#       print(\"batch id: \", batch_id)\n",
        "#       print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "      print('\\nEpoch {}/{} Cost: {:.6f} Train Acc {}'.format(e+1, num_epochs, loss.item(), train_acc / len(train_dataloader)))\n",
        "      trainacclist.append(train_acc / (batch_id+1))\n",
        "      model.eval()\n",
        "      for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(notebook.tqdm(test_dataloader)):\n",
        "          token_ids = token_ids.long().to(device)\n",
        "          segment_ids = segment_ids.long().to(device)\n",
        "          valid_length= valid_length\n",
        "          label = label.long().to(device)\n",
        "          out = model(token_ids, valid_length, segment_ids)\n",
        "          test_acc += calc_accuracy(out, label)\n",
        "      # print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
        "      print('\\nEpoch {}/{} Cost: {:.6f} Test Acc {}'.format(e+1, num_epochs, loss.item(), test_acc / len(test_dataloader)))\n",
        "      testacclist.append(test_acc / (batch_id+1))\n",
        "      newaccu = test_acc / (batch_id+1)\n",
        "      if newaccu > oldaccu:\n",
        "        oldaccu = newaccu\n",
        "        torch.save(model, model_name)\n",
        "      gc.collect()  \n",
        "  plt.figure()\n",
        "  print(trainacclist)\n",
        "  trainarray.append(trainacclist[-1])\n",
        "  print(testacclist)\n",
        "  testarray.append(testacclist[-1])\n",
        "  x = np.arange(len(trainacclist))\n",
        "  plt.plot(x,trainacclist,'b',label='train')\n",
        "  plt.plot(x,testacclist,'g',label='validate')\n",
        "  # plt.axis('equal')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('acc')\n",
        "  plt.title('SNS dataset with caption')\n",
        "  # plt.setp(lines, color='r', linewidth=2.0)\n",
        "  plt.legend()\n",
        "  plt.savefig('{}.png'.format(nsplit),format='png')\n",
        "  nsplit +=1\n",
        "\n",
        "plt.figure()\n",
        "x = np.arange(len(trainarray))\n",
        "plt.plot(x,trainarray,'b',label='train')\n",
        "plt.plot(x,testarray,'g',label='validate')\n",
        "# plt.axis('equal')\n",
        "plt.xlabel('K Folds')\n",
        "plt.ylabel('avg. acc')\n",
        "plt.title('SNS dataset with caption according to K-Folds')\n",
        "# plt.setp(lines, color='r', linewidth=2.0)\n",
        "plt.legend()\n",
        "plt.savefig('{}.png'.format(nsplit),format='png')\n",
        "nsplit +=1\n",
        "\n",
        "print(np.array(trainarray))\n",
        "print(np.array(testarray))\n",
        "\n",
        "# pd.DataFrame(np.array(trainarray)).to_csv('trainaccu.csv')\n",
        "# pd.DataFrame(np.array(testarray)).to_csv('testaccu.csv')"
      ],
      "metadata": {
        "id": "TmXGz8-GXRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_K (X,Y,K, np_testdata):\n",
        "  max_vals1,max_indicesK = torch.topk(X,K)\n",
        "  # print(max_indicesK)\n",
        "  precisionnum = 0\n",
        "  index = 0\n",
        "  for yi, maxindicesi in zip(Y,max_indicesK):\n",
        "    precisionnum += yi in maxindicesi\n",
        "    if yi != maxindicesi:\n",
        "        print(\"\")\n",
        "        print(yi, maxindicesi)\n",
        "        print(np_testdata[index])\n",
        "        print(\"\")\n",
        "    index += 1\n",
        "  \n",
        "  #maxlength = max_indicesK.size()[0] #precision_acc\n",
        "  return precisionnum"
      ],
      "metadata": {
        "id": "LhgOwF7aXS5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = BERTDataset(np_testdata, 1, 0, tok, max_len, True, False)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=num_workers) \n",
        "gc.collect()\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# tensor = torch.ones(())\n",
        "# wholelabel =tensor.new_empty()\n",
        "# wholeout =tensor.new_empty()\n",
        "wholelabel =[]\n",
        "wholeout =[]\n",
        "categorylist = [\"화장품\",\"패션\",\"요리음식\",\"여행아웃도어\",\"인테리어\",\"엔터테인먼트\",\"육아\",\"아이티\",\"자동차\",\"헬스/피트니스\",\"반려동물\"]\n",
        "modelbest = torch.load(model_name)\n",
        "modelbest.to(device)\n",
        "modelbest.eval()\n",
        "precision_at_3 =0.0\n",
        "test_acc1 =0.0\n",
        "start = 0\n",
        "end = start + batch_size\n",
        "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(notebook.tqdm(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = modelbest(token_ids, valid_length, segment_ids)\n",
        "        labellist = [categorylist[labeli] for labeli in label]\n",
        "        outlist = [categorylist[outi.argmax()] for outi in out]\n",
        "        wholelabel+=labellist\n",
        "        wholeout+=outlist\n",
        "        test_acc1 += calc_accuracy(out, label)\n",
        "        precision_at_3 += precision_at_K (out,label,1, np_testdata[start:end])\n",
        "        start += batch_size\n",
        "        end = start + batch_size\n",
        "\n",
        "# print(test_acc1*(batch_id+1)/len(data_test))\n",
        "print(precision_at_3)\n",
        "# print(totallength)\n",
        "print((len(data_test)))\n",
        "print(\"Total Acc\", precision_at_3/len(data_test))\n",
        "confusion_matrix(wholelabel, wholeout,labels= categorylist)\n",
        "np.save('confusionmatrix',confusion_matrix(wholelabel, wholeout,labels= categorylist))\n",
        "confusion_matrix(wholelabel, wholeout,labels= categorylist)"
      ],
      "metadata": {
        "id": "YX2dGGxMXTw4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}